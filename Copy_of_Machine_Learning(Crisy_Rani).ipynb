{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crissy09/Machine_Learning_Crissy_Rani/blob/main/Copy_of_Machine_Learning(Crisy_Rani).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10,6)\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ],
      "metadata": {
        "id": "GQVRgdNqcHgK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "CSV_PATH = '/content/drive/MyDrive/codon_usage.csv'\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "bci-3LXPd-VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define meta and codon columns\n",
        "meta_cols = ['Kingdom', 'DNAtype', 'SpeciesID', 'Ncodons', 'SpeciesName']\n",
        "codon_cols = [c for c in df.columns if c not in meta_cols]\n",
        "print(\"Meta columns:\", meta_cols)\n",
        "print(\"Number of codon feature columns:\", len(codon_cols))"
      ],
      "metadata": {
        "id": "efFYLCDxuPRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "_YSj-dZDv3sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check for missing values**"
      ],
      "metadata": {
        "id": "HZ8a6UkyvwFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MISSING VALUES\")\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Values': missing_values,\n",
        "    'Percentage': missing_percentage\n",
        "})\n",
        "print(missing_df[missing_df['Missing Values'] > 0])\n"
      ],
      "metadata": {
        "id": "PkdDjxSfhzC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outlier Handling**"
      ],
      "metadata": {
        "id": "JTXF7OcNvLtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BOXPLOTS BEFORE outlier handling\n",
        "df_num = df[codon_cols].apply(pd.to_numeric, errors='coerce')\n",
        "sample_cols = codon_cols[:6]\n",
        "plt.figure()\n",
        "sns.boxplot(data=df_num[sample_cols])\n",
        "plt.title(\"Boxplots BEFORE Outlier Handling (numeric coercion applied)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# IQR capping function and application (winsorize)\n",
        "\n",
        "def cap_iqr(series):\n",
        "    q1 = series.quantile(0.25)\n",
        "    q3 = series.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    return series.clip(lower, upper)\n",
        "\n",
        "df_capped = df_num.copy()\n",
        "for col in codon_cols:\n",
        "    if df_capped[col].notna().sum() == 0:\n",
        "        continue\n",
        "    df_capped[col] = cap_iqr(df_capped[col])\n",
        "\n",
        "# BOXPLOTS AFTER outlier handling\n",
        "plt.figure()\n",
        "sns.boxplot(data=df_capped[sample_cols])\n",
        "plt.title(\"Boxplots AFTER Outlier Handling (IQR capped)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jpW107fAk1kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Imputation**"
      ],
      "metadata": {
        "id": "QFgudPf5v9g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute any remaining NaNs in codon features (mean imputation)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_capped[codon_cols] = imputer.fit_transform(df_capped[codon_cols])\n",
        "print(\"Mean imputation done for codon features.\")\n",
        "print(\"Remaining NaNs in codon features:\", df_capped[codon_cols].isna().sum().sum())"
      ],
      "metadata": {
        "id": "VsjUpOxaufnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Encoding for Target Variable**"
      ],
      "metadata": {
        "id": "25-RzYRomcNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_col = 'Kingdom'\n",
        "df_capped[target_col] = df[target_col]\n",
        "\n",
        "le = LabelEncoder()\n",
        "df_capped['y'] = le.fit_transform(df_capped[target_col].astype(str))\n",
        "\n",
        "print(\"Target classes:\", list(le.classes_))\n",
        "print(\"Encoded mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n"
      ],
      "metadata": {
        "id": "jBjXXhfPmae_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Scaling**"
      ],
      "metadata": {
        "id": "9ROfF9Mbmjhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_capped[codon_cols].astype(float)\n",
        "y = df_capped['y']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Scaling completed. Sample scaled values (first row, first 10 features):\")\n",
        "print(X_scaled[0][:10])"
      ],
      "metadata": {
        "id": "uqBZc2_LmfLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting data into training and testing**"
      ],
      "metadata": {
        "id": "9P6NJD8mmup7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.20, random_state=42, stratify=y\n",
        ")\n",
        "print(\"Train/Test shapes:\", X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "MrBqgsn-msHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Kingdom Classes\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(x=df['Kingdom'], palette=\"viridis\")\n",
        "plt.title(\"Distribution of Kingdom Classes\")\n",
        "plt.xlabel(\"Kingdom\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NnvljZclLhsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Matrix\n",
        "codon_numeric = df[codon_cols].apply(pd.to_numeric, errors='coerce')\n",
        "plt.figure(figsize=(16,12))\n",
        "corr = codon_numeric.corr()\n",
        "sns.heatmap(corr, cmap='coolwarm', linewidths=0.2)\n",
        "plt.title(\"Correlation Matrix of Codon Frequency Features (64 Codons)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_1EZ2dguLrMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLASSIFICATION MODELS"
      ],
      "metadata": {
        "id": "Z7Nn2B_imz0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOGISTIC REGRESSION**"
      ],
      "metadata": {
        "id": "x26BV6c3sLL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_lr)\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.title(\"Logistic Regression - Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zxE4dLqXok3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LINEAR DISCRIMINANT ANALYSIS**"
      ],
      "metadata": {
        "id": "dyMxZSv2sIBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LinearDiscriminantAnalysis()\n",
        "lda.fit(X_train, y_train)\n",
        "y_pred_lda = lda.predict(X_test)\n",
        "\n",
        "print(\"Linear Discriminant Analysis Accuracy:\", round(accuracy_score(y_test, y_pred_lda),4))\n",
        "print(classification_report(y_test, y_pred_lda))\n",
        "cm = confusion_matrix(y_test, y_pred_lda)\n",
        "plt.figure(figsize=(7,6)); sns.heatmap(cm, annot=True, fmt='d', cmap='BuPu'); plt.title(\"LDA - Confusion Matrix\"); plt.show()\n"
      ],
      "metadata": {
        "id": "HcJaedsbo8KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DECISION TREE**"
      ],
      "metadata": {
        "id": "qHLeZGe7sBp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", round(accuracy_score(y_test, y_pred_dt),4))\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "cm = confusion_matrix(y_test, y_pred_dt)\n",
        "plt.figure(figsize=(7,6)); sns.heatmap(cm, annot=True, fmt='d', cmap='Purples'); plt.title(\"Decision Tree - Confusion Matrix\"); plt.show()\n"
      ],
      "metadata": {
        "id": "WelF8lt1pOLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HYPERPARAMETER TUNING: LOGISTIC REGRESSION (GridSearchCV)**"
      ],
      "metadata": {
        "id": "dBdJhcd8xK4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['lbfgs'],\n",
        "    'max_iter': [300]\n",
        "}\n",
        "\n",
        "logreg = LogisticRegression(\n",
        "    random_state=42,\n",
        "    multi_class='multinomial',\n",
        "    warm_start=True\n",
        ")\n",
        "\n",
        "gs = GridSearchCV(\n",
        "    logreg, param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "gs.fit(X_train, y_train)\n",
        "\n",
        "best_lr = gs.best_estimator_\n",
        "print(\"Best Logistic Regression params:\", gs.best_params_)\n",
        "print(\"Best CV score:\", round(gs.best_score_, 4))\n",
        "\n",
        "y_pred_best = best_lr.predict(X_test)\n",
        "print(\"\\nTUNED LOGISTIC REGRESSION - Test Accuracy:\", round(accuracy_score(y_test, y_pred_best), 4))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best, target_names=le.classes_))\n",
        "\n",
        "# Confusion matrix plot\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.title(\"Tuned Logistic Regression - Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
        "plt.xticks(rotation=45); plt.yticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yGtPFtEFw6F6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}